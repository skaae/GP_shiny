{
    "contents" : "%!TEX root=../master.tex\n\\section{Gaussian Processes}\nA Gaussian process is defined as a probability distribution over functions f(x) such that f(x) evaluated at any \nset of points $x_1...x_n$ is jointly Gaussian.\n\nWe define the gaussian process, $\\mathcal{G}\\mathcal{P}$ with:\n\\begin{eqnarray}\n &f(x) &\\sim \\mathcal{G}\\mathcal{P}(m(x),k(x,x')) \\label{eq:gp_def1}\\\\\n \\text{Mean}& m(x) &= E[f(x)]  \\label{eq:gp_def2} \\\\\n \\text{Covariance}& k(x,x') &= E[(f(x) .m(x))(f(x')-m(x'))]   \\label{eq:gp_def3} \\\\\n X &\\in \\Re^D\n\\end{eqnarray}\nWe often asssumes that $m(x) = 0$. $f(x)$ is the process evaluated at the point $\\mathbf{x}$\nUsing the definition of a $\\mathcal{G}\\mathcal{P}$ we can draw a number of functions, $\\mathbf{f}_*$ from a particular\n$\\mathcal{G}\\mathcal{P}$ evaluated at the points $X_*$, i.e:\n\\begin{equation}\n\t\\mathbf{f}_* \\sim \\mathcal{N}(0,K(X_*,X_*))\n\t\\label{eq:gp_f}\n\\end{equation}\nThe following is a short demonstration of \\ref{eq:gp_f}. In order to draw functions from the $\\mathcal{G}\\mathcal{P}$\nwe need to defines its covariance functions. This examples uses the squarred exponential covariance function:\n\\begin{equation}\n\t\\label{eq:sqr_exp}\n\tk(x,x') = \\sigma_f \\cdot \\exp\\left(\\frac{-(x-x')^2}{2 \\cdot l^2}\\right)\n\\end{equation}\nHere $\\sigma_f$ is magnitude parameter governing the overall variability of the process\nand $l$ is the length scale governing the governing the correlation between points. Both $\\sigma_f$ and $l$ \nhyperparameters of the $\\mathcal{G}\\mathcal{P}$.\n\nTo draw functions from the $\\mathcal{G}\\mathcal{P}$:\n\\begin{enumerate}\n\t\\item Create a vector of test inputs, $X_*$ at which to evaluate the function\n\t\\item calculate the covariance matrix, $K(X_*,X_*)$ with the covariance function $k(x,x')$, e.g with \n\t\tequation \\ref{eq:sqr_exp} or some other valid kernel function. \n\t\tSee figure \\ref{fig:f_draw} panel C) for example covariance function.\n\t\\item Draw multivariate samples from multivariate function $\\mathbf{f}_*\\sim\\mathcal{N}(0,K(X_*,X_*))$.\n\t\t\tWhere $\\mathbf{f}_*$ is evaluation of the $\\mathcal{G}\\mathcal{P}$ at the test points $X_*$\n\\end{enumerate}\n\nThe code in listing \\ref{lst:f_gp} and listing \\ref{lst:calc_k} was used to draw functions from a $\\mathcal{G}\\mathcal{P}$\nusing equation \\ref{eq:gp_f}. The plots from listing \\ref{lst:f_gp} is shown in figure \\ref{fig:f_draw}.\nA) shows 3 functions drawn from the $\\mathcal{G}\\mathcal{P}$, B) shows $f(x)$ evaluated at $x=-3.1633$ 5000 times, \nthe plot shows that f(x) is gaussian. Lastly C) shows the covariance matrix.\n\n\\begin{figure}[htbp]\n\t\\begin{center}\n\t\t\\includegraphics[width=1\\textwidth]{figs/f_samples.eps}\n\t\\end{center}\n\t\\caption{A) shows 3 functions drawn from the $\\mathcal{G}\\mathcal{P}$, B) shows $f(x)$ evaluated at $x=-3.1633$ 5000 times, \n\tpanel B is overlayed with a standard normal distribution, which shows that f(x) evaluated at $x=-3.1633$ is gaussian. \n\tLastly C) shows the covariance matrix using the squarred exponential kernel.}\n\t\\label{fig:f_draw}\n\\end{figure}%\n%\n%\n\\begin{lstlisting}[frame=single,language=matlab,caption=Draw functios from GP,\nlabel=lst:f_gp,language=matlab]\nsetSeed(12345);\nx_star = linspace(-5,5,50);  % test data\nsigma_f= 1; l = 1; f_samples = 5000;, sigma_n = 0.5; \n\n% squarred exp kernel\nsqr_exp = @(x1,x2) sigma_f * exp(-(x1-x2)^2 / 2*l^2);\nkernel = sqr_exp\nK = calc_k(x_star,x_star,0,sqr_exp);\ngp_samples = mvnrnd(zeros(1,length(K)), K, f_samples);\n\n%% plots\nsubplot(1,3,1); plot(repmat(x_star',1,3),...\n                     gp_samples(1:3,:)','LineWidth',2)\nhold on; plot([x_star(10) x_star(10)],[2,-2.5],'k-'); hold off;\nylabel('f(x)'),xlabel('x'),title('A')\n\nsubplot(1,3,2); \nbinWidth = 0.7; %This is the bin width\nbinCtrs = -3:binWidth:3; %Bin centers, depends on your data\nn=length(gp_samples(:,10));\ncounts = hist(gp_samples(:,10),binCtrs);\nprob = counts / (n * binWidth);\nH = bar(binCtrs,prob,'hist');\nset(H,'facecolor',[0.5 0.5 0.5]); hold on;\ngaus = normpdf(-3:.1:3,0,K(10,10)); %requires Statistics toolbox\nplot(-3:.1:3,gaus,'r','linewidth',3);\nxlabel('f(x)'); ylabel('Freq'),title('B')\nhold off;\n\nsubplot(1,3,3); imagesc(x_star,x_star,K/max(max(K)));\ncolormap('gray');\nylabel('x'),xlabel('x_prime'),title('C');\n\\end{lstlisting}\n\\begin{lstlisting}[frame=single,language=matlab,caption=Function for calculation covariance matrix,\nlabel=lst:calc_k,language=matlab]\nfunction [ K ] = calc_k(x1,x2,noise,kernel )\n% Calculate covariance \nK = zeros(length(x1),length(x2));\nI = eye(size(K));\nfor i = 1:size(K,1)\n    for j = 1:size(K,2)    \n        K(i,j) = kernel(x1(i),x2(j));   \n    end\nend\nK = K + I .* noise ;\nend\n\\end{lstlisting}%\n%\n%\n%\n\\subsection{$\\mathcal{G}\\mathcal{P}$ regression} \\FloatBarrier\nIn the regression setting we are intrestred in drawning functions from the  $\\mathcal{G}\\mathcal{P}$.\nFirst step in the inference step is to condition the functions drawn from the GP on the training data \n$\\mathcal{D} = \\{(x_1,y_1),...m(x_n,y_n)\\}$, where $X$ is the training input and $\\mathbf{y}$ is the \ntraining output. We assume that our observations of the targets $y$ are corrupted by noise, i.e the observation model is assumed to be:\n\\begin{eqnarray}\n\t\\mathbf{y} = \\mathbf{f} + \\mathbf{\\epsilon}, \\epsilon \\sim \\mathcal{N}(0,I\\sigma_n^2)\n\t\\label{eq:gp_regress}\t \n\\end{eqnarray}\nBecause the noise is assumed to be independent of $\\mathbf{f}$ we use that sum of \nindependent gaussian the sum of the means and the sum of the covariances, i.e\n\\begin{eqnarray}\n     \\mathbf{y} &\\sim& \\mathcal{N}(0,K(X,X) + I\\sigma_n^2) \\label{eq:y_dist}\\\\\n    \\mathbf{y}|\\mathbf{f} &\\sim& \\mathcal{N}(\\mathbf{f},I\\sigma_n^2)\n\\end{eqnarray}\nFollowing the notaion of \\cite{Rasmussen:2006vz}, we have that the joint distribution \nbetween our observed targets, $\\mathbf{y}$ and the points that we want to evaluate \n$\\mathbf{f}_*$ is given by:\n\\begin{equation}\n\t\\begin{bmatrix}\\mathbf{y} \\\\ \\mathbf{f}_*\\end{bmatrix} \\sim \\mathcal{N}\\left(0,\n\\begin{bmatrix}K(X,X)+\\sigma_n^2I & K(X,X_*)\\\\ K(X_*,X) & K(X_*,X_*)\\end{bmatrix}\n\\right)\n\\end{equation}%\n%\nTo get the distribution of $\\mathbf{f}_*$ we use standard rules of conditioning\n\\footnote{See \\cite{Rasmussen:2006vz} appendix A.2, p. 200, \n\\url{http://www.gaussianprocess.org/gpml/chapters/RWA.pdf}} on mulitivariate gaussians, which gives the distribution of functions conditioned on the test data ($X_*$), training data ($X$) and the observed targets ($\\mathbf{y}$):\n\\begin{eqnarray}\n\t\\mathbf{f}_* | X,\\mathbf{y},X_* &\\sim& \\mathcal{N}(\\bar{\\mathbf{f}}_*,cov(\\mathbf{f_*})) \\label{eq:yf_cond} \\\\\n\t\\bar{\\mathbf{f}}_*  &=& K(X_*,X)[K(X,X)+\\sigma^2_nI]^{-1}\\mathbf{y} \\\\\n\tcov(\\mathbf{f_*}) &=& K(X_*,X_*) - K(X_*,X)[K(X,X)+\\sigma^2_nI]^{-1}K(X,X_*)\n\\end{eqnarray}%\n%\n%\nThe equations above are used in listing \\ref{lst:f_samples_cond} where we first condition on \nnoisefree data and then on data including noise.\\\\\\\\%\n%\nFigure \\ref{fig:f_samples_cond} shows the figure produced by listing \\ref{lst:f_samples_cond}.\nNote than in panel A) all functions passes through the observed data because we have\nassumed that the observations has no noise. In panel B) noise is added and the functions\nare allowed not to move through the observations. Panel C) shows the mean of $\\mathbf{f}_* | X,\\mathbf{y},X_*$\nwhere the shade indicate uncertainty.\n\\begin{figure}[htbp]\n\t\\begin{center}\n\t\t\\includegraphics[width=1\\textwidth]{figs/f_samples_cond.eps}\n\t\\end{center}\n\t\\caption{A) $\\mathbf{f}_* | X,\\mathbf{y},X_*$ using noise free observations, \n\t\t\tB) shows $\\mathbf{f}_* | X,\\mathbf{y},X_*$ using observations with nose\n\t\t\t$\\sigma^2_n$, and C) shows $\\mathbf{\\bar{f}}_*$ using observations with noise. In C) The \n\t\t\tshaded error is the uncertainty.}\n\t\\label{fig:f_samples_cond}\n\\end{figure}\n\\begin{lstlisting}[frame=single,language=matlab,caption=conditioning f on data,\nlabel=lst:f_samples_cond,language=matlab]\n%% samples conditioned on observed data\n% calculate mean and covariance of training data (2.23 and 2.24)\n% calculate k see Rasmussen (2.21)\n% Observed data\nx         = [-4,-3,-1,0,2];\ny         = [-2,0,1,2,-1]';\n\nk_xx_nonoise     = calc_k(x,x,0,kernel);\nk_xx     = calc_k(x,x,sigma_n,kernel);\nk_xxs    = calc_k(x,x_star,0,kernel);\nk_xsx    = calc_k(x_star,x,0,kernel);\nk_xsxs   = calc_k(x_star,x_star,0,kernel);\n\nf_mean_s = k_xsx * inv(k_xx_nonoise) * y;\nf_cov_s  = k_xsxs - k_xsx * inv(k_xx_nonoise) * k_xxs;\ngp_samples_wdata_nonoise = mvnrnd(f_mean_s,f_cov_s,3);\n\nf_mean_s = k_xsx * inv(k_xx) * y;\nf_cov_s  = k_xsxs - k_xsx * inv(k_xx) * k_xxs;\ngp_samples_wdata = mvnrnd(f_mean_s,f_cov_s,3);\n\nfigure(2)\nsubplot(1,3,1); plot(repmat(x_star',1,3),gp_samples_wdata_nonoise', ...\n                    'LineWidth',2);\nylabel('f(x)'), xlabel('x'); title('A')\nhold on; plot(x,y,'bo','LineWidth',5); hold off;\nsubplot(1,3,2); plot(repmat(x_star',1,3),gp_samples_wdata',...\n                      'LineWidth',2)\n                  \nhold on; errorbar(x,y,ones(1,length(x)).*sigma_n,'.'); \nplot(x,y,'bo','LineWidth',5); hold off;\nxlabel('x'); title('B')\n\nsubplot(1,3,3); shadedErrorBar(x_star,f_mean_s,diag(f_cov_s)); hold on;\nplot(x_star,f_mean_s,'r','LineWidth',2); hold off;\nxlabel('x'); title('C')\n\\end{lstlisting}\n\n\\subsection{Tuning the hyper parameters} \\FloatBarrier\nThe marginal likelihood (evidence) is defined as:\n\\begin{eqnarray}\n\tp(\\mathbf{y}|X) &=& \n    \\int \\text{lik.} \\times \\text{fun. prior}=\n    \\int p(\\mathbf{y}|\\mathbf{f},X)p(\\mathbf{f}|X)d\\mathbf{f} \\\\\n\t&\\mathbf{f}&\\text{: training output} \\\\\n\t&\\mathbf{f}_* &\\text{: test output} \n\\end{eqnarray}\nWhere marginal refers to marginalization over the function values $\\mathbf{f}$.\nEquation \\ref{eq:gp_def1} shows the definition of a \n$\\mathcal{G}\\mathcal{P}$, assuming that the mean is 0 we\nget the prior distribution: \n\\begin{equation}\n    p(\\mathbf{f}|X) \\sim \\mathcal{N}(0,K(X,X))\n    \\label{eq:gp_f_post}\n\\end{equation}\nThe likelihood of the targets, $\\mathbf{y}$, are shown in equation \\ref{eq:gp_regress} and equation \\ref{eq:y_dist},\nthe likelihood is\n\\begin{equation}\n    \\mathbf{y}|\\mathbf{f} \\sim \\mathcal{N}(\\mathbf{f}, I\\sigma_n^2)\n\\end{equation}\n%$$eq:y_dist$\n\nWe now that the distribution of our observed functions values, $\\mathbf{y}$\nis $\\mathbf{y}\\sim \\mathcal{N}(0,K(X,X)+\\sigma_n^2I)$. \nTo optimize this we find the log marginal likelihood as:\n\\begin{eqnarray}\n\t\\mathbf{y}\\sim \\mathcal{N}(0,K(X,X)+\\sigma_n^2I) = 2\\pi^{-\\frac{n}{2}}\\left|K+\\sigma_n^2\\right|^{-\\frac{1}{2}}\n\te^{\\left(-\\frac{1}{2}\\mathbf{y}^T\\left(K+\\sigma_n^2\\right)\\mathbf{y}\\right)} \\\\\n\t\\ln\\mathcal(l) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|K+\\sigma_n^2I| -\\frac{1}{2}\\mathbf{y}^T\\left(K+\\sigma_n^2I\\right)\\mathbf{y}\n\\end{eqnarray}\n\nListing \\ref{lst:f_samples_hyper} shows how the hyperparameters can be found using grid search.\nIn the example the observation noise $\\sigma_n^2$ is keept at 0.1 and the maximum marginal likelihood\nis found. The result of running listing \\ref{lst:f_samples_hyper} is shown in \nfigure \\ref{fig:f_samples_hyper}. The left panel shows a contour plot of the negative loglikelihood, \nwhere the red dot shows the minimum. The middle plot shows three functions drawn from the\nfunction posterior and the right plot shows uncertainty and mean prediction.\n\\begin{lstlisting}[frame=single,language=matlab,caption=conditioning f on data,\nlabel=lst:f_samples_hyper,language=matlab]\n%% varying the hyper parameters\ns_n = 0.1;\nresolution = 200;\ns_f = linspace(0,5,resolution);\nl_f = linspace(0.00000,2,resolution);\n\nmarg_loglik = @(n,K,y,s_n) -0.5*n*log(2*pi)...\n-0.5*log( det( K+eye(length(K))*s_n) ) ...\n-0.5*y'* inv(K+eye(length(K))*s_n)*y;\ngrid_mll = zeros(resolution);\nfor i = 1:length(s_f)\n    for j = 1:length(l_f)\n      sqr_exp = @(x1,x2) s_f(i) * exp(-(x1-x2)^2 / 2*l_f(j)^2);\n      K = calc_k(x,x,s_n,sqr_exp);  \n      n = length(y);\n      grid_mll(i,j) = marg_loglik(n,K,y,s_n);\n    end\nend\nfprintf('\\n');\n\n% plot negative loglikeihood, normalized\n% we need to find the minimum\nfigure(3); subplot(1,3,1);\nim = -grid_mll ./ max(max(-grid_mll));\ncontour(l_f,s_f,im);\nylabel('length'); xlabel('\\sigma_f^2'); title('negative normalized log lik')\n[v,ind]=min(im(:));\n[sf_minidx,lf_minidx] = ind2sub(size(im),ind);\nlf_min = l_f(lf_minidx); \nsf_min = s_f(sf_minidx);\nhold on;\nplot(lf_min,sf_min,'or','LineWidth',6);\nhold off;\n\nkernel = @(x1,x2) sf_min * exp(-(x1-x2)^2 / 2*lf_min^2);\nk_xx     = calc_k(x,x,s_n,kernel);\nk_xxs    = calc_k(x,x_star,0,kernel);\nk_xsx    = calc_k(x_star,x,0,kernel);\nk_xsxs   = calc_k(x_star,x_star,0,kernel);\n\nf_mean_s = k_xsx * inv(k_xx) * y;\nf_cov_s  = k_xsxs - k_xsx * inv(k_xx) * k_xxs;\ngp_samples_wdata = mvnrnd(f_mean_s,f_cov_s,3);\n\nsubplot(1,3,2); plot(repmat(x_star',1,3),gp_samples_wdata',...\n                      'LineWidth',2)\nhold on; errorbar(x,y,ones(1,length(x)).*s_n,'.'); hold off;\nxlabel('x'); title('3 functions drawn from Posterior')\n\nsubplot(1,3,3); shadedErrorBar(x_star,f_mean_s,diag(f_cov_s)); hold on;\nplot(x_star,f_mean_s,'r','LineWidth',2); \nplot(x,y,'b.')\nhold off;\nxlabel('x'); title('Uncertainty and mean')\n\\end{lstlisting}\n\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=1\\textwidth]{figs/f_samples_hyper.eps}\n\t\\end{center}\n\t\\caption{The left panel shows a contour plot of the negative loglikelihood, \nwhere the red dot shows the minimum. The middle plot shows three functions drawn from the\nfunction posterior and the right plot shows uncertainty and mean prediction.}\n\t\\label{fig:f_samples_hyper}\n\\end{figure}",
    "created" : 1387283208655.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4000646427",
    "id" : "14F97C5A",
    "lastKnownWriteTime" : 1387283735,
    "path" : "~/Documents/Speciale/breast_cancer/documents/GP.tex",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "tex"
}